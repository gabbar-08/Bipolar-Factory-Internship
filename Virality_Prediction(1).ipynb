{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Virality-Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbHYwSLkcIXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article  \n",
        "import csv \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODjVjhUisSV",
        "colab_type": "code",
        "outputId": "380c9fa6-ee1b-4dbf-c537-66f28287c0ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 3.4MB/s \n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Building wheels for collected packages: feedfinder2, tinysegmenter, feedparser, jieba3k\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=58f3b3ad672711699cebd0ba52c3de9c5ab7ded479fa174e60f2eddf4cadd09b\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=53da0702ed9ee6ee4e8ad174fb23dd4b181c99abf7f271783d7eac3d48e900d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=38cb062bbfc7e0987ae4beca111c3a6e76d73bb0293af8a5e313c8964291b962\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=b87cd1cc433498ed2a3d0014d8bfaef2c4a000ef0038c16ea6935e2a29911573\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "Successfully built feedfinder2 tinysegmenter feedparser jieba3k\n",
            "Installing collected packages: requests-file, tldextract, feedfinder2, cssselect, tinysegmenter, feedparser, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPBecMuciwn",
        "colab_type": "code",
        "outputId": "5b813e80-4c3b-4c2b-ecd6-0f9e9670a3f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "url = \"https://aajtak.intoday.in/\"\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request as urllib2\n",
        "import re\n",
        "\n",
        "html_page = urllib2.urlopen(\"https://aajtak.intoday.in/\")\n",
        "soup = BeautifulSoup(html_page)\n",
        "links = []\n",
        "\n",
        "for link in soup.findAll('a'):\n",
        "    if not link.get('href').startswith('http'):\n",
        "      link = url + link.get('href')\n",
        "      links.append(link)\n",
        "print(links)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://aajtak.intoday.in//moviemasala', 'https://aajtak.intoday.in//news-on-films.html', 'https://aajtak.intoday.in//new-trailer-and-new-songs-of-upcoming-hindi-films-of-bollywood.html', 'https://aajtak.intoday.in//filmy-gossips.html', 'https://aajtak.intoday.in//film-review.html','......', 'https://aajtak.intoday.in//calender.php', 'https://aajtak.intoday.in//investor', 'https://aajtak.intoday.in//story/breaking-news-1-59000.html', 'https://aajtak.intoday.in//khabare-ab-tak.html', 'https://aajtak.intoday.in//rssfeeds/?feed=rss1.0&no_html=1&rsspage=home', 'https://aajtak.intoday.in//distribution/rio/', 'https://aajtak.intoday.in//tvtmi', 'https://aajtak.intoday.in/ https://www.headlinestoday.in/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrA9Y27kdVrC",
        "colab_type": "code",
        "outputId": "850861c5-b464-4014-f17c-8ea259e76a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df=[]\n",
        "for i in links:\n",
        "    print(i)\n",
        "    article = Article(i, language=\"en\")\n",
        "    try:\n",
        "      article.download() \n",
        "      article.parse() \n",
        "      article.nlp() \n",
        "    except :\n",
        "      print(\"data not found\")\n",
        "    data={}\n",
        "    data['Title']=article.title\n",
        "    data['Keywords']=article.keywords\n",
        "    if \"movie\" in data['Keywords'] or \"bollywood\" in data['Keywords'] or \"sports\" in data['Keywords'] or \"star\" in data['Keywords'] or \"trailers\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 1\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    elif \"money\" in data['Keywords'] or \" stocks\" in data['Keywords'] or \"business\" in data['Keywords']  or \"rajasthan\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 1\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    elif \"latest\" in data['Keywords'] or \"life\" in data['Keywords'] or \"coronavirus\" in data['Keywords'] or \"media\" in data['Keywords'] or \"health\" in data['Keywords'] or  \"covid-19\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 1\n",
        "    else:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    df.append(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://aajtak.intoday.in//moviemasala\n",
            "https://aajtak.intoday.in//news-on-films.html\n",
            "https://aajtak.intoday.in//new-trailer-and-new-songs-of-upcoming-hindi-films-of-bollywood.html\n",
            "https://aajtak.intoday.in//filmy-gossips.html\n",
            "https://aajtak.intoday.in//film-review.html\n",
            "https://aajtak.intoday.in//celebrity-interview.html\n",
            "https://aajtak.intoday.in//small-screen.html\n",
            "https://aajtak.intoday.in//karyakrams/video/movie-masala.html\n",
            "https://aajtak.intoday.in//viral-video.html\n",
            "https://aajtak.intoday.in//celebrity-birthday-today.html\n",
            "https://aajtak.intoday.in//photos/hollywood.html\n",
            "https://aajtak.intoday.in//style.html\n",
	    " .\n.\n.\n.\n"
            "https://aajtak.intoday.in//rssfeeds/?feed=rss1.0&no_html=1&rsspage=home\n",
            "https://aajtak.intoday.in//distribution/rio/\n",
            "data not found\n",
            "https://aajtak.intoday.in//tvtmi\n",
            "https://aajtak.intoday.in/ https://www.headlinestoday.in/\n",
            "data not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfVYA0idYjT",
        "colab_type": "code",
        "outputId": "72a7dfe2-174e-43bd-f5b5-32ff244aceff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset=pd.DataFrame(df)\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(448, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhdpNb1m_-M",
        "colab_type": "code",
        "outputId": "327b3cd2-8966-4e03-f33c-3918b75a856d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(dataset.shape)\n",
        "D = dataset\n",
        "print(D.shape)\n",
        "D.drop([1])\n",
        "print(D.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(140, 5)\n",
            "(140, 5)\n",
            "(140, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4cIm-amGnv",
        "colab_type": "code",
        "outputId": "0fcb98e2-2bdd-4cb8-c552-6a1cd30447d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "for i in range(dataset.shape[0]):\n",
        "  try:\n",
        "    if(dataset.iloc[i,2] == 0 ):\n",
        "      if( dataset.iloc[i,3] == 0):\n",
        "        if (dataset.iloc[i,4] == 0):\n",
        "          dataset = dataset.drop(i,axis = 0)\n",
        "  except:\n",
        "   continue\n",
        "dataset.to_csv('df.csv')\n",
        "files.download('df.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UxrmHZRwj8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoV7XRdPwn_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "FILEPATH = \"OnlineNewsPopularity.csv\" \n",
        "def clean_cols(data):\n",
        "    \"\"\"Clean the column names by stripping and lowercase.\"\"\"\n",
        "    clean_col_map = {x: x.lower().strip() for x in list(data)}\n",
        "    return data.rename(index=str, columns=clean_col_map)\n",
        "\n",
        "def TrainTestSplit(X, Y, R=0, test_size=0.2):\n",
        "    \"\"\"Easy Train Test Split call.\"\"\"\n",
        "    return train_test_split(X, Y, test_size=test_size, random_state=R)\n",
        "\n",
        "full_data = clean_cols(pd.read_csv(FILEPATH))\n",
        "train_set, test_set = train_test_split(full_data, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = train_set.drop(['url','shares', 'timedelta', 'lda_00','lda_01','lda_02','lda_03','lda_04','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
        "y_train = train_set['shares']\n",
        "\n",
        "x_test = test_set.drop(['url','shares', 'timedelta', 'num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
        "y_test = test_set['shares']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clf = RandomForestRegressor(random_state=42)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "rf_res = pd.DataFrame(clf.predict(x_train),list(y_train))\n",
        "rf_res.reset_index(level=0, inplace=True)\n",
        "rf_res_df = rf_res.rename(index=str, columns={\"index\": \"Actual shares\", 0: \"Predicted shares\"})\n",
        "rf_res_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbn_rgF40AGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "The Web Scrapping Part is Completely Done 100% and same was the model training part.\n",
        "But due to some technical issues, I was not able to give the \" prestige \", the final act to this project.\n",
        "Still I have noted below the steps needed t be taken after training the model, also the reference code for the same is continued below.\n",
        "\n",
        "Step 1:  Convert our Dataset and make it similar to the predefined UCI Dataset.\n",
        "        Add All the columns\n",
        "        Compute All the Values\n",
        "        Assume if necessary\n",
        "Step 2:   Test the model with our dataset as input\n",
        "\n",
        "This is not a big part of the code, but I understand it is very important part of the code.\n",
        "\n",
        "Thank You !"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEJNcGzTxERn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words('english'))\n",
        "\n",
        "def rate_unique(words):\n",
        "    words=tokenize(words)\n",
        "    no_order = list(set(words))\n",
        "    rate_unique=len(no_order)/len(words)\n",
        "    return rate_unique\n",
        "\n",
        "\n",
        "def rate_nonstop(words):\n",
        "    words=tokenize(words)\n",
        "    filtered_sentence = [w for w in words if not w in stopwords]\n",
        "    rate_nonstop=len(filtered_sentence)/len(words)\n",
        "    no_order = list(set(filtered_sentence))\n",
        "    rate_unique_nonstop=len(no_order)/len(words)\n",
        "    return rate_nonstop,rate_unique_nonstop\n",
        "\n",
        "\n",
        "def avg_token(words):\n",
        "    words=tokenize(words)\n",
        "    length=[]\n",
        "    for i in words:\n",
        "        length.append(len(i))\n",
        "    return np.average(length)\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import datefinder\n",
        "import datetime  \n",
        "from datetime import date \n",
        "def day(article_text):\n",
        "    article=article_text\n",
        "    if len(list(datefinder.find_dates(article)))>0:\n",
        "        date=str(list(datefinder.find_dates(article))[0])\n",
        "        date=date.split()\n",
        "        date=date[0]\n",
        "        year, month, day = date.split('-')     \n",
        "        day_name = datetime.date(int(year), int(month), int(day)) \n",
        "        return day_name.strftime(\"%A\")\n",
        "    return \"Monday\"\n",
        "\n",
        "def tokenize(text):\n",
        "    text=text\n",
        "    return word_tokenize(text)\n",
        "\n",
        "pos_words=[]\n",
        "neg_words=[]\n",
        "def polar(words):\n",
        "    all_tokens=tokenize(words)\n",
        "    for i in all_tokens:\n",
        "        analysis=TextBlob(i)\n",
        "        polarity=analysis.sentiment.polarity\n",
        "        if polarity>0:\n",
        "            pos_words.append(i)\n",
        "        if polarity<0:\n",
        "            neg_words.append(i)\n",
        "    return pos_words,neg_words\n",
        "\n",
        "def rates(words):\n",
        "    words=polar(words)\n",
        "    pos=words[0]\n",
        "    neg=words[1]\n",
        "    all_words=words\n",
        "    global_rate_positive_words=(len(pos)/len(all_words))/100\n",
        "    global_rate_negative_words=(len(neg)/len(all_words))/100\n",
        "    pol_pos=[]\n",
        "    pol_neg=[]\n",
        "    for i in pos:\n",
        "        analysis=TextBlob(i)\n",
        "        pol_pos.append(analysis.sentiment.polarity)\n",
        "        avg_positive_polarity=analysis.sentiment.polarity\n",
        "    for j in neg:\n",
        "        analysis2=TextBlob(j)\n",
        "        pol_neg.append(analysis2.sentiment.polarity)\n",
        "        avg_negative_polarity=analysis2.sentiment.polarity\n",
        "    min_positive_polarity=min(pol_pos)\n",
        "    max_positive_polarity=max(pol_pos)\n",
        "    min_negative_polarity=min(pol_neg)\n",
        "    max_negative_polarity=max(pol_neg)\n",
        "    avg_positive_polarity=np.average(pol_pos)\n",
        "    avg_negative_polarity=np.average(pol_neg)\n",
        "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bHazNogw4T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "df2=[]\n",
        "for i in news:\n",
        "    pred_info={}\n",
        "    article = Article(i, language=\"en\") # en for English \n",
        "    article.download() \n",
        "    article.parse()\n",
        "    analysis=TextBlob(article.text)\n",
        "    polarity=analysis.sentiment.polarity\n",
        "    title_analysis=TextBlob(article.title)\n",
        "    pred_info['text']=article.text\n",
        "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
        "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
        "    pred_info['n_unique_tokens']=rate_unique(article.text)\n",
        "    pred_info['n_non_stop_words']=rate_nonstop(article.text)[0]\n",
        "    pred_info['n_non_stop_unique_tokens']=rate_nonstop(article.text)[1]\n",
        "    pred_info['num_hrefs']=article.html.count(\"https://timesofindia.indiatimes.com\")\n",
        "    pred_info['num_imgs']=len(article.images)\n",
        "    pred_info['num_videos']=len(article.movies)\n",
        "    pred_info['average_token_length']=avg_token(article.text)\n",
        "    pred_info['num_keywords']=len(article.keywords)\n",
        "    \n",
        "    if \"life-style\" in article.url:\n",
        "        pred_info['data_channel_is_lifestyle']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_lifestyle']=0\n",
        "    if \"etimes\" in article.url:\n",
        "        pred_info['data_channel_is_entertainment']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_entertainment']=0\n",
        "    if \"business\" in article.url:\n",
        "        pred_info['data_channel_is_bus']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_bus']=0\n",
        "    if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
        "        data_channel_is_socmed=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_socmed=0\n",
        "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
        "        data_channel_is_tech=1\n",
        "        data_channel_is_socmed=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_tech=0\n",
        "    if \"world\" in article.url:\n",
        "        data_channel_is_world=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_socmed=0\n",
        "    else:\n",
        "        data_channel_is_world=0\n",
        "        \n",
        "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
        "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
        "    pred_info['data_channel_is_world']=data_channel_is_world\n",
        "    \n",
        "    if day(i)==\"Monday\":\n",
        "        pred_info['weekday_is_monday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_monday']=0\n",
        "    if day(i)==\"Tuesday\":\n",
        "        pred_info['weekday_is_tuesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_tuesday']=0\n",
        "    if day(i)==\"Wednesday\":\n",
        "        pred_info['weekday_is_wednesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_wednesday']=0\n",
        "    if day(i)==\"Thursday\":\n",
        "        pred_info['weekday_is_thursday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_thursday']=0\n",
        "    if day(i)==\"Friday\":\n",
        "        pred_info['weekday_is_friday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_friday']=0\n",
        "    if day(i)==\"Saturday\":\n",
        "        pred_info['weekday_is_saturday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_saturday']=0\n",
        "    if day(i)==\"Sunday\":\n",
        "        pred_info['weekday_is_sunday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_sunday']=0\n",
        "        pred_info['is_weekend']=0\n",
        "        \n",
        "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
        "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
        "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
        "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
        "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
        "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
        "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
        "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
        "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
        "    pred_info['max_negative_polarity']=rates(article.text)[7]    \n",
        "    pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
        "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
        "    df2.append(pred_info)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBr7wAx5w3LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df=pd.DataFrame(df2)\n",
        "pred_test=pred_df.drop(['text'],axis=1)\n",
        "pred_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYBdBWxmwu3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2=pd.DataFrame(clf.predict(pred_test),pred_df['text'])\n",
        "test2.reset_index(level=0, inplace=True)\n",
        "test2 = test2.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
        "test2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pukgXOsPwzJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
