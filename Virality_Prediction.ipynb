{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Virality-Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbHYwSLkcIXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article  \n",
        "import csv \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LODjVjhUisSV",
        "colab_type": "code",
        "outputId": "51c4d13f-bb24-4078-929b-ee1baa8cf97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPBecMuciwn",
        "colab_type": "code",
        "outputId": "4ff58aa7-bfe5-4447-bf7a-2d83ba12989e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "url = \"https://aajtak.intoday.in/\"\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request as urllib2\n",
        "import re\n",
        "\n",
        "html_page = urllib2.urlopen(\"https://aajtak.intoday.in/\")\n",
        "soup = BeautifulSoup(html_page)\n",
        "links = []\n",
        "\n",
        "for link in soup.findAll('a'):\n",
        "    if not link.get('href').startswith('http'):\n",
        "      link = url + link.get('href')\n",
        "      links.append(link)\n",
        "print(links[:5])\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://aajtak.intoday.in//moviemasala', 'https://aajtak.intoday.in//news-on-films.html', 'https://aajtak.intoday.in//new-trailer-and-new-songs-of-upcoming-hindi-films-of-bollywood.html', 'https://aajtak.intoday.in//filmy-gossips.html', 'https://aajtak.intoday.in//film-review.html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrA9Y27kdVrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=[]\n",
        "for i in links:\n",
        "    article = Article(i, language=\"en\")\n",
        "    try:\n",
        "      article.download() \n",
        "      article.parse() \n",
        "      article.nlp() \n",
        "    except :\n",
        "      continue\n",
        "    data={}\n",
        "    data['Title']=article.title\n",
        "    data['Keywords']=article.keywords\n",
        "    if \"movie\" in data['Keywords'] or \"bollywood\" in data['Keywords'] or \"sports\" in data['Keywords'] or \"star\" in data['Keywords'] or \"trailers\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 1\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    elif \"money\" in data['Keywords'] or \" stocks\" in data['Keywords'] or \"business\" in data['Keywords']  or \"rajasthan\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 1\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    elif \"latest\" in data['Keywords'] or \"life\" in data['Keywords'] or \"coronavirus\" in data['Keywords'] or \"media\" in data['Keywords'] or \"health\" in data['Keywords'] or  \"covid-19\" in data['Keywords']:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 1\n",
        "    else:\n",
        "      data['data_channel_is_entertainment'] = 0\n",
        "      data[' data_channel_is_bus'] = 0\n",
        "      data['data_channel_is_lifestyle'] = 0\n",
        "    df.append(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfVYA0idYjT",
        "colab_type": "code",
        "outputId": "72a7dfe2-174e-43bd-f5b5-32ff244aceff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset=pd.DataFrame(df)\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(448, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhdpNb1m_-M",
        "colab_type": "code",
        "outputId": "327b3cd2-8966-4e03-f33c-3918b75a856d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(dataset.shape)\n",
        "D = dataset\n",
        "print(D.shape)\n",
        "D.drop([1])\n",
        "print(D.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(140, 5)\n",
            "(140, 5)\n",
            "(140, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4cIm-amGnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for i in range(dataset.shape[0]):\n",
        "  try:\n",
        "    if(dataset.iloc[i,2] == 0 ):\n",
        "      if( dataset.iloc[i,3] == 0):\n",
        "        if (dataset.iloc[i,4] == 0):\n",
        "          dataset = dataset.drop(i,axis = 0)\n",
        "  except:\n",
        "   continue\n",
        "dataset.to_csv('df.csv')\n",
        "files.download('df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UxrmHZRwj8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoV7XRdPwn_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "FILEPATH = \"OnlineNewsPopularity.csv\" \n",
        "def clean_cols(data):\n",
        "    \"\"\"Clean the column names by stripping and lowercase.\"\"\"\n",
        "    clean_col_map = {x: x.lower().strip() for x in list(data)}\n",
        "    return data.rename(index=str, columns=clean_col_map)\n",
        "\n",
        "def TrainTestSplit(X, Y, R=0, test_size=0.2):\n",
        "    \"\"\"Easy Train Test Split call.\"\"\"\n",
        "    return train_test_split(X, Y, test_size=test_size, random_state=R)\n",
        "\n",
        "full_data = clean_cols(pd.read_csv(FILEPATH))\n",
        "train_set, test_set = train_test_split(full_data, test_size=0.20, random_state=42)\n",
        "\n",
        "x_train = train_set.drop(['url','shares', 'timedelta', 'lda_00','lda_01','lda_02','lda_03','lda_04','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
        "y_train = train_set['shares']\n",
        "\n",
        "x_test = test_set.drop(['url','shares', 'timedelta', 'num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
        "y_test = test_set['shares']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clf = RandomForestRegressor(random_state=42)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "rf_res = pd.DataFrame(clf.predict(x_train),list(y_train))\n",
        "rf_res.reset_index(level=0, inplace=True)\n",
        "rf_res_df = rf_res.rename(index=str, columns={\"index\": \"Actual shares\", 0: \"Predicted shares\"})\n",
        "rf_res_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbn_rgF40AGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "The Web Scrapping Part is Completely Done 100% and same was the model training part.\n",
        "But due to some technical issues, I was not able to give the \" prestige \", the final act to this project.\n",
        "Still I have noted below the steps needed t be taken after training the model, also the reference code for the same is continued below.\n",
        "\n",
        "Step 1:  Convert our Dataset and make it similar to the predefined UCI Dataset.\n",
        "        Add All the columns\n",
        "        Compute All the Values\n",
        "        Assume if necessary\n",
        "Step 2:   Test the model with our dataset as input\n",
        "\n",
        "This is not a big part of the code, but I understand it is very important part of the code.\n",
        "\n",
        "Thank You !"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEJNcGzTxERn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "stopwords=set(stopwords.words('english'))\n",
        "\n",
        "def rate_unique(words):\n",
        "    words=tokenize(words)\n",
        "    no_order = list(set(words))\n",
        "    rate_unique=len(no_order)/len(words)\n",
        "    return rate_unique\n",
        "\n",
        "\n",
        "def rate_nonstop(words):\n",
        "    words=tokenize(words)\n",
        "    filtered_sentence = [w for w in words if not w in stopwords]\n",
        "    rate_nonstop=len(filtered_sentence)/len(words)\n",
        "    no_order = list(set(filtered_sentence))\n",
        "    rate_unique_nonstop=len(no_order)/len(words)\n",
        "    return rate_nonstop,rate_unique_nonstop\n",
        "\n",
        "\n",
        "def avg_token(words):\n",
        "    words=tokenize(words)\n",
        "    length=[]\n",
        "    for i in words:\n",
        "        length.append(len(i))\n",
        "    return np.average(length)\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import datefinder\n",
        "import datetime  \n",
        "from datetime import date \n",
        "def day(article_text):\n",
        "    article=article_text\n",
        "    if len(list(datefinder.find_dates(article)))>0:\n",
        "        date=str(list(datefinder.find_dates(article))[0])\n",
        "        date=date.split()\n",
        "        date=date[0]\n",
        "        year, month, day = date.split('-')     \n",
        "        day_name = datetime.date(int(year), int(month), int(day)) \n",
        "        return day_name.strftime(\"%A\")\n",
        "    return \"Monday\"\n",
        "\n",
        "def tokenize(text):\n",
        "    text=text\n",
        "    return word_tokenize(text)\n",
        "\n",
        "pos_words=[]\n",
        "neg_words=[]\n",
        "def polar(words):\n",
        "    all_tokens=tokenize(words)\n",
        "    for i in all_tokens:\n",
        "        analysis=TextBlob(i)\n",
        "        polarity=analysis.sentiment.polarity\n",
        "        if polarity>0:\n",
        "            pos_words.append(i)\n",
        "        if polarity<0:\n",
        "            neg_words.append(i)\n",
        "    return pos_words,neg_words\n",
        "\n",
        "def rates(words):\n",
        "    words=polar(words)\n",
        "    pos=words[0]\n",
        "    neg=words[1]\n",
        "    all_words=words\n",
        "    global_rate_positive_words=(len(pos)/len(all_words))/100\n",
        "    global_rate_negative_words=(len(neg)/len(all_words))/100\n",
        "    pol_pos=[]\n",
        "    pol_neg=[]\n",
        "    for i in pos:\n",
        "        analysis=TextBlob(i)\n",
        "        pol_pos.append(analysis.sentiment.polarity)\n",
        "        avg_positive_polarity=analysis.sentiment.polarity\n",
        "    for j in neg:\n",
        "        analysis2=TextBlob(j)\n",
        "        pol_neg.append(analysis2.sentiment.polarity)\n",
        "        avg_negative_polarity=analysis2.sentiment.polarity\n",
        "    min_positive_polarity=min(pol_pos)\n",
        "    max_positive_polarity=max(pol_pos)\n",
        "    min_negative_polarity=min(pol_neg)\n",
        "    max_negative_polarity=max(pol_neg)\n",
        "    avg_positive_polarity=np.average(pol_pos)\n",
        "    avg_negative_polarity=np.average(pol_neg)\n",
        "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bHazNogw4T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "df2=[]\n",
        "for i in news:\n",
        "    pred_info={}\n",
        "    article = Article(i, language=\"en\") # en for English \n",
        "    article.download() \n",
        "    article.parse()\n",
        "    analysis=TextBlob(article.text)\n",
        "    polarity=analysis.sentiment.polarity\n",
        "    title_analysis=TextBlob(article.title)\n",
        "    pred_info['text']=article.text\n",
        "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
        "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
        "    pred_info['n_unique_tokens']=rate_unique(article.text)\n",
        "    pred_info['n_non_stop_words']=rate_nonstop(article.text)[0]\n",
        "    pred_info['n_non_stop_unique_tokens']=rate_nonstop(article.text)[1]\n",
        "    pred_info['num_hrefs']=article.html.count(\"https://timesofindia.indiatimes.com\")\n",
        "    pred_info['num_imgs']=len(article.images)\n",
        "    pred_info['num_videos']=len(article.movies)\n",
        "    pred_info['average_token_length']=avg_token(article.text)\n",
        "    pred_info['num_keywords']=len(article.keywords)\n",
        "    \n",
        "    if \"life-style\" in article.url:\n",
        "        pred_info['data_channel_is_lifestyle']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_lifestyle']=0\n",
        "    if \"etimes\" in article.url:\n",
        "        pred_info['data_channel_is_entertainment']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_entertainment']=0\n",
        "    if \"business\" in article.url:\n",
        "        pred_info['data_channel_is_bus']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_bus']=0\n",
        "    if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
        "        data_channel_is_socmed=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_socmed=0\n",
        "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
        "        data_channel_is_tech=1\n",
        "        data_channel_is_socmed=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_tech=0\n",
        "    if \"world\" in article.url:\n",
        "        data_channel_is_world=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_socmed=0\n",
        "    else:\n",
        "        data_channel_is_world=0\n",
        "        \n",
        "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
        "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
        "    pred_info['data_channel_is_world']=data_channel_is_world\n",
        "    \n",
        "    if day(i)==\"Monday\":\n",
        "        pred_info['weekday_is_monday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_monday']=0\n",
        "    if day(i)==\"Tuesday\":\n",
        "        pred_info['weekday_is_tuesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_tuesday']=0\n",
        "    if day(i)==\"Wednesday\":\n",
        "        pred_info['weekday_is_wednesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_wednesday']=0\n",
        "    if day(i)==\"Thursday\":\n",
        "        pred_info['weekday_is_thursday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_thursday']=0\n",
        "    if day(i)==\"Friday\":\n",
        "        pred_info['weekday_is_friday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_friday']=0\n",
        "    if day(i)==\"Saturday\":\n",
        "        pred_info['weekday_is_saturday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_saturday']=0\n",
        "    if day(i)==\"Sunday\":\n",
        "        pred_info['weekday_is_sunday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_sunday']=0\n",
        "        pred_info['is_weekend']=0\n",
        "        \n",
        "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
        "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
        "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
        "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
        "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
        "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
        "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
        "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
        "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
        "    pred_info['max_negative_polarity']=rates(article.text)[7]    \n",
        "    pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
        "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
        "    df2.append(pred_info)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBr7wAx5w3LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df=pd.DataFrame(df2)\n",
        "pred_test=pred_df.drop(['text'],axis=1)\n",
        "pred_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYBdBWxmwu3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2=pd.DataFrame(clf.predict(pred_test),pred_df['text'])\n",
        "test2.reset_index(level=0, inplace=True)\n",
        "test2 = test2.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
        "test2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pukgXOsPwzJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}